# üèÜ Innofusion'26 Hackathon - ML Project Architecture

## üìã Project Overview

**Project Name**: Customer Purchase Intent Classification  
**Competition**: Innofusion'26 Data Science Hackathon  
**Task**: Section 4 - Machine Learning Pipeline  
**Problem Type**: Multiclass Classification (4 classes)  
**Target Variable**: Purchase_Intent (Need-based, Impulsive, Planned, Wants-based)

---

## üìÅ Project Structure

```
innofusion-hackathon/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ Propmt.md                                          # Requirements specification
‚îú‚îÄ‚îÄ üìÑ README.md                                          # General project info
‚îú‚îÄ‚îÄ üìÑ PROJECT_ARCHITECTURE.md                            # This document
‚îÇ
‚îú‚îÄ‚îÄ üìä Ecommerce_Consumer_Behavior_Analysis_Data.csv      # Raw dataset (27 columns)
‚îÇ
‚îú‚îÄ‚îÄ üêç section_4_ml_notebook.py                           # Main ML pipeline (Colab-ready)
‚îÇ
‚îú‚îÄ‚îÄ üìÇ outputs/                                           # Generated artifacts
‚îÇ   ‚îú‚îÄ‚îÄ model_comparison.png                              # Model accuracy comparison chart
‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix.png                              # Best model confusion matrix
‚îÇ   ‚îî‚îÄ‚îÄ feature_importance.png                            # Feature importance analysis
‚îÇ
‚îî‚îÄ‚îÄ üìÇ .vscode/                                           # VS Code settings (optional)
```

---

## üèóÔ∏è Architecture Overview

### **Architecture Type**: Sequential ML Pipeline

This project implements a **linear, end-to-end machine learning pipeline** designed for Google Colab execution. The architecture follows a sequential flow without complex abstractions, making it beginner-friendly and easy to debug.

---

## üîÑ Data Flow Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      RAW DATA LAYER                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  File: Ecommerce_Consumer_Behavior_Analysis_Data.csv            ‚îÇ
‚îÇ  Rows: ~N samples                                               ‚îÇ
‚îÇ  Cols: 27 features (mixed types)                                ‚îÇ
‚îÇ        ‚îú‚îÄ‚îÄ 3 columns to drop                                    ‚îÇ
‚îÇ        ‚îú‚îÄ‚îÄ 1 column to clean (Purchase_Amount: "$XX" ‚Üí float)  ‚îÇ
‚îÇ        ‚îú‚îÄ‚îÄ 2 boolean columns (TRUE/FALSE ‚Üí 1/0)                ‚îÇ
‚îÇ        ‚îú‚îÄ‚îÄ 13 categorical columns (to encode)                   ‚îÇ
‚îÇ        ‚îî‚îÄ‚îÄ 1 target column (Purchase_Intent)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   DATA PREPROCESSING LAYER                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚úì Drop: Customer_ID, Location, Time_of_Purchase               ‚îÇ
‚îÇ  ‚úì Clean: Purchase_Amount (remove "$" and spaces)              ‚îÇ
‚îÇ  ‚úì Convert: Discount_Used, Customer_Loyalty_Program_Member     ‚îÇ
‚îÇ  ‚úì Encode: 13 categorical features using LabelEncoder          ‚îÇ
‚îÇ  ‚úì Encode: Target variable (save encoder for inverse mapping)  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Output: Clean DataFrame (24 columns, all numeric)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  FEATURE ENGINEERING LAYER                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Feature Selection: 11 key features                            ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Numerical (6): Age, Purchase_Amount,                      ‚îÇ
‚îÇ  ‚îÇ                   Frequency_of_Purchase,                    ‚îÇ
‚îÇ  ‚îÇ                   Customer_Satisfaction, Brand_Loyalty,     ‚îÇ
‚îÇ  ‚îÇ                   Product_Rating,                           ‚îÇ
‚îÇ  ‚îÇ                   Time_Spent_on_Product_Research(hours)     ‚îÇ
‚îÇ  ‚îÇ                                                             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Categorical Encoded (4): Discount_Sensitivity,            ‚îÇ
‚îÇ                                Income_Level,                    ‚îÇ
‚îÇ                                Engagement_with_Ads,             ‚îÇ
‚îÇ                                Social_Media_Influence           ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Target: Purchase_Intent (4 classes)                           ‚îÇ
‚îÇ  Split: 80% train / 20% test (stratified)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    MODEL TRAINING LAYER                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì  ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì  ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì  ‚îÇ
‚îÇ  ‚îÉ   Logistic      ‚îÉ  ‚îÉ  Random Forest  ‚îÉ  ‚îÉ    XGBoost    ‚îÉ  ‚îÇ
‚îÇ  ‚îÉ   Regression    ‚îÉ  ‚îÉ   Classifier    ‚îÉ  ‚îÉ   Classifier  ‚îÉ  ‚îÇ
‚îÇ  ‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´  ‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´  ‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´  ‚îÇ
‚îÇ  ‚îÉ max_iter: 1000  ‚îÉ  ‚îÉ n_estimators:   ‚îÉ  ‚îÉ n_estimators: ‚îÉ  ‚îÇ
‚îÇ  ‚îÉ random_state:42 ‚îÉ  ‚îÉ 100             ‚îÉ  ‚îÉ 200           ‚îÉ  ‚îÇ
‚îÇ  ‚îÉ                 ‚îÉ  ‚îÉ max_depth: 10   ‚îÉ  ‚îÉ max_depth: 4  ‚îÉ  ‚îÇ
‚îÇ  ‚îÉ Purpose:        ‚îÉ  ‚îÉ class_weight:   ‚îÉ  ‚îÉ learning_rate:‚îÉ  ‚îÇ
‚îÇ  ‚îÉ Baseline model  ‚îÉ  ‚îÉ balanced        ‚îÉ  ‚îÉ 0.1           ‚îÉ  ‚îÇ
‚îÇ  ‚îÉ                 ‚îÉ  ‚îÉ random_state:42 ‚îÉ  ‚îÉ subsample:0.8 ‚îÉ  ‚îÇ
‚îÇ  ‚îÉ                 ‚îÉ  ‚îÉ                 ‚îÉ  ‚îÉ random_state: ‚îÉ  ‚îÇ
‚îÇ  ‚îÉ                 ‚îÉ  ‚îÉ Purpose:        ‚îÉ  ‚îÉ 42            ‚îÉ  ‚îÇ
‚îÇ  ‚îÉ                 ‚îÉ  ‚îÉ Feature         ‚îÉ  ‚îÉ               ‚îÉ  ‚îÇ
‚îÇ  ‚îÉ                 ‚îÉ  ‚îÉ importance      ‚îÉ  ‚îÉ Purpose:      ‚îÉ  ‚îÇ
‚îÇ  ‚îÉ                 ‚îÉ  ‚îÉ analysis        ‚îÉ  ‚îÉ Best accuracy ‚îÉ  ‚îÇ
‚îÇ  ‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ  ‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ  ‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚Üí All models trained on same train/test split                 ‚îÇ
‚îÇ  ‚Üí Predictions generated for test set                          ‚îÇ
‚îÇ  ‚Üí Accuracies stored: lr_accuracy, rf_accuracy, xgb_accuracy   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  MODEL EVALUATION LAYER                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  1. Compare Accuracies                                          ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ Identify best model automatically                       ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  2. Detailed Evaluation of Best Model                           ‚îÇ
‚îÇ     ‚îú‚îÄ‚îÄ Confusion Matrix (with actual class names)             ‚îÇ
‚îÇ     ‚îú‚îÄ‚îÄ Classification Report (precision/recall/F1)            ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ 5-Fold Cross Validation                                ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  3. Feature Importance Analysis                                 ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ Extract from Random Forest (regardless of best model)   ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Metrics Tracked:                                               ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Test Accuracy                                              ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Cross-Validation Mean ¬± Std                               ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Per-Class Precision, Recall, F1-Score                     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Feature Importance Scores                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              VISUALIZATION & INSIGHTS LAYER                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  üìä Chart 1: model_comparison.png                              ‚îÇ
‚îÇ     ‚îú‚îÄ‚îÄ Type: Horizontal bar chart                             ‚îÇ
‚îÇ     ‚îú‚îÄ‚îÄ Data: 3 model accuracies                               ‚îÇ
‚îÇ     ‚îú‚îÄ‚îÄ Colors: ['#4C72B0', '#55A868', '#C44E52']              ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ Features: Dashed line at max accuracy                  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  üìä Chart 2: confusion_matrix.png                              ‚îÇ
‚îÇ     ‚îú‚îÄ‚îÄ Type: Seaborn heatmap                                  ‚îÇ
‚îÇ     ‚îú‚îÄ‚îÄ Data: Best model predictions vs actual                 ‚îÇ
‚îÇ     ‚îú‚îÄ‚îÄ Color: 'Blues' colormap                                ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ Labels: Actual class names (inverse transformed)       ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  üìä Chart 3: feature_importance.png                            ‚îÇ
‚îÇ     ‚îú‚îÄ‚îÄ Type: Horizontal bar chart                             ‚îÇ
‚îÇ     ‚îú‚îÄ‚îÄ Data: Random Forest feature importances                ‚îÇ
‚îÇ     ‚îú‚îÄ‚îÄ Color: '#4C72B0'                                       ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ Sort: Descending by importance                         ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  üìã Business Summary Box                                        ‚îÇ
‚îÇ     ‚îú‚îÄ‚îÄ Best model name and accuracy                           ‚îÇ
‚îÇ     ‚îú‚îÄ‚îÄ Cross-validation scores                                ‚îÇ
‚îÇ     ‚îú‚îÄ‚îÄ Top 3 features                                         ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ Auto-generated business insights                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        OUTPUT LAYER                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Files Generated:                                               ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ outputs/model_comparison.png                              ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ outputs/confusion_matrix.png                              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ outputs/feature_importance.png                            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Console Outputs:                                               ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Class mapping (0‚ÜíImpulsive, 1‚ÜíNeed-based, etc.)          ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Train/test split sizes                                    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Model accuracies                                           ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Classification report (precision/recall/F1)               ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Cross-validation results                                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Business summary box                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Technical Stack

| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| **Language** | Python | 3.8+ | Core programming |
| **Notebook** | Google Colab | Latest | Execution environment |
| **Data Processing** | pandas | Latest | Data manipulation |
| **Numerical Computing** | numpy | Latest | Array operations |
| **Visualization** | matplotlib | Latest | Chart generation |
| **Statistical Viz** | seaborn | Latest | Heatmaps & advanced plots |
| **ML Framework** | scikit-learn | Latest | Classical ML models |
| **Gradient Boosting** | xgboost | Latest | XGBoost classifier |

---

## üéØ Design Principles

### 1. **Simplicity First**
- No custom classes or complex abstractions
- Sequential code blocks (cell-by-cell execution)
- Beginner-friendly comments throughout

### 2. **Reproducibility**
- `random_state=42` in all stochastic operations
- Fixed train/test split ratio (80/20)
- Stratified sampling for class balance

### 3. **Modularity**
- Each section is independent
- Clear section headers with visual separators
- Success messages after each section

### 4. **Visual Clarity**
- Minimum figure size: (10, 6)
- Consistent color schemes
- Descriptive titles and axis labels
- Saved as high-quality PNG files

### 5. **Business Focus**
- Auto-generated insights from model results
- Clear summary box with key metrics
- Feature importance analysis for decision-making

---

## üìä Data Architecture

### **Input Schema**
```
Dataset: Ecommerce_Consumer_Behavior_Analysis_Data.csv
‚îú‚îÄ‚îÄ Total Columns: 27
‚îú‚îÄ‚îÄ Total Rows: ~N (varies by dataset version)
‚îÇ
‚îú‚îÄ‚îÄ Columns to Drop (3):
‚îÇ   ‚îú‚îÄ‚îÄ Customer_ID        : Unique identifier (no predictive value)
‚îÇ   ‚îú‚îÄ‚îÄ Location           : Text field (too granular)
‚îÇ   ‚îî‚îÄ‚îÄ Time_of_Purchase   : Date string (timing not in scope)
‚îÇ
‚îú‚îÄ‚îÄ Columns to Clean (1):
‚îÇ   ‚îî‚îÄ‚îÄ Purchase_Amount    : "$XXX.XX " ‚Üí float (remove $ and spaces)
‚îÇ
‚îú‚îÄ‚îÄ Columns to Convert (2):
‚îÇ   ‚îú‚îÄ‚îÄ Discount_Used                      : "TRUE"/"FALSE" ‚Üí 1/0
‚îÇ   ‚îî‚îÄ‚îÄ Customer_Loyalty_Program_Member    : "TRUE"/"FALSE" ‚Üí 1/0
‚îÇ
‚îú‚îÄ‚îÄ Categorical Columns to Encode (13):
‚îÇ   ‚îú‚îÄ‚îÄ Gender
‚îÇ   ‚îú‚îÄ‚îÄ Income_Level
‚îÇ   ‚îú‚îÄ‚îÄ Marital_Status
‚îÇ   ‚îú‚îÄ‚îÄ Education_Level
‚îÇ   ‚îú‚îÄ‚îÄ Occupation
‚îÇ   ‚îú‚îÄ‚îÄ Purchase_Category
‚îÇ   ‚îú‚îÄ‚îÄ Purchase_Channel
‚îÇ   ‚îú‚îÄ‚îÄ Social_Media_Influence
‚îÇ   ‚îú‚îÄ‚îÄ Discount_Sensitivity
‚îÇ   ‚îú‚îÄ‚îÄ Engagement_with_Ads
‚îÇ   ‚îú‚îÄ‚îÄ Device_Used_for_Shopping
‚îÇ   ‚îú‚îÄ‚îÄ Payment_Method
‚îÇ   ‚îî‚îÄ‚îÄ Shipping_Preference
‚îÇ
‚îú‚îÄ‚îÄ Target Variable (1):
‚îÇ   ‚îî‚îÄ‚îÄ Purchase_Intent     : (Need-based, Impulsive, Planned, Wants-based)
‚îÇ
‚îî‚îÄ‚îÄ Other Features (7 numerical):
    ‚îú‚îÄ‚îÄ Age
    ‚îú‚îÄ‚îÄ Frequency_of_Purchase
    ‚îú‚îÄ‚îÄ Brand_Loyalty
    ‚îú‚îÄ‚îÄ Product_Rating
    ‚îú‚îÄ‚îÄ Time_Spent_on_Product_Research(hours)
    ‚îú‚îÄ‚îÄ Return_Rate
    ‚îú‚îÄ‚îÄ Customer_Satisfaction
    ‚îî‚îÄ‚îÄ Time_to_Decision
```

### **Feature Selection Strategy**
Selected **11 features** covering entire customer journey:

1. **Demographics** (2 features)
   - Age
   - Income_Level (encoded)

2. **Financial Behavior** (2 features)
   - Purchase_Amount
   - Discount_Sensitivity (encoded)

3. **Engagement Metrics** (3 features)
   - Social_Media_Influence (encoded)
   - Engagement_with_Ads (encoded)
   - Time_Spent_on_Product_Research(hours)

4. **Purchase Behavior** (2 features)
   - Frequency_of_Purchase
   - Brand_Loyalty

5. **Satisfaction Metrics** (2 features)
   - Customer_Satisfaction
   - Product_Rating

---

## ü§ñ Model Architecture

### **Model 1: Logistic Regression**
```python
LogisticRegression(
    max_iter=1000,        # Sufficient for convergence
    random_state=42       # Reproducibility
)
```
- **Purpose**: Baseline model
- **Complexity**: Low
- **Interpretability**: High
- **Expected Accuracy**: 45-55%

### **Model 2: Random Forest**
```python
RandomForestClassifier(
    n_estimators=100,      # 100 decision trees
    max_depth=10,          # Prevent overfitting
    random_state=42,       # Reproducibility
    class_weight='balanced' # Handle class imbalance
)
```
- **Purpose**: Feature importance analysis
- **Complexity**: Medium
- **Interpretability**: Medium (via feature importance)
- **Expected Accuracy**: 60-70%

### **Model 3: XGBoost**
```python
XGBClassifier(
    n_estimators=200,      # More trees = better performance
    max_depth=4,           # Shallow trees (prevent overfit)
    learning_rate=0.1,     # Standard learning rate
    subsample=0.8,         # 80% of samples per tree
    colsample_bytree=0.8,  # 80% of features per tree
    use_label_encoder=False, # Suppress warning
    eval_metric='mlogloss', # Multiclass log loss
    random_state=42        # Reproducibility
)
```
- **Purpose**: Best accuracy (likely winner)
- **Complexity**: High
- **Interpretability**: Low
- **Expected Accuracy**: 65-75%+

---

## üìà Evaluation Metrics

### **Primary Metric**
- **Accuracy**: Proportion of correct predictions
  - Used for model comparison
  - Simple and interpretable

### **Secondary Metrics**
- **Precision**: True Positives / (True Positives + False Positives)
  - Per-class precision reported
- **Recall**: True Positives / (True Positives + False Negatives)
  - Per-class recall reported
- **F1-Score**: Harmonic mean of precision and recall
  - Balanced metric for each class

### **Validation Strategy**
- **Train-Test Split**: 80/20 with stratification
- **Cross-Validation**: 5-fold on best model
  - Reports mean ¬± standard deviation
  - Checks model stability

---

## üé® Visualization Standards

### **Chart 1: Model Comparison**
```
Type: Horizontal Bar Chart
Size: (10, 6)
Colors: ['#4C72B0', '#55A868', '#C44E52']
        (Blue, Green, Red)
X-axis: Accuracy (0.0 to 1.0)
Y-axis: Model Names
Special: Dashed vertical line at max accuracy
Labels: Accuracy values displayed on bars
```

### **Chart 2: Confusion Matrix**
```
Type: Seaborn Heatmap
Size: (10, 6)
Colormap: 'Blues'
Annot: True (show counts in cells)
Fmt: 'd' (integer format)
Labels: Actual class names (inverse transformed)
Title: "Confusion Matrix - [Best Model Name]"
```

### **Chart 3: Feature Importance**
```
Type: Horizontal Bar Chart
Size: (10, 6)
Color: '#4C72B0' (Blue)
Sort: Descending by importance score
X-axis: Feature Importance Score
Y-axis: Feature Names
Labels: Importance values displayed on bars
```

---

## üîê Code Quality Standards

### **Comments**
```python
# ‚îÄ‚îÄ SECTION X.X: TITLE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Brief description of what this section does

# Inline comment for important operations
variable = operation()  # Brief explanation if needed

print("‚úÖ Section X.X complete")
```

### **Naming Conventions**
- **Variables**: `snake_case` (e.g., `lr_accuracy`)
- **Constants**: `UPPER_SNAKE_CASE` (if any)
- **Descriptive**: `best_model` not `bm`

### **Error Handling**
- Assume clean data (hackathon context)
- No try-except blocks (keep code simple)
- Let errors surface for debugging

---

## üöÄ Execution Flow

### **Sequential Execution (10 Steps)**
```
Step 1: Install Dependencies
  ‚îî‚îÄ‚îÄ !pip install xgboost -q

Step 2: Import Libraries
  ‚îî‚îÄ‚îÄ All imports at top

Step 3-12: Execute Sections 4.1 through 4.10
  ‚îú‚îÄ‚îÄ Each section prints success message
  ‚îú‚îÄ‚îÄ Sections are independent
  ‚îî‚îÄ‚îÄ Can be run as separate Colab cells

Final: Complete Message
  ‚îî‚îÄ‚îÄ "üéâ Machine Learning Pipeline Complete!"
```

### **Estimated Runtime**
- **Data Loading**: ~1-2 seconds
- **Preprocessing**: ~2-3 seconds
- **Model Training**: ~10-30 seconds (all 3 models)
- **Evaluation/Visualization**: ~5-10 seconds
- **Total**: ~20-50 seconds (single Colab run)

---

## üì§ Deliverables Checklist

### **Code Deliverable**
- [x] `section_4_ml_notebook.py` (Colab-ready Python script)

### **Visual Outputs**
- [x] `model_comparison.png` (3 models compared)
- [x] `confusion_matrix.png` (best model evaluation)
- [x] `feature_importance.png` (RF importance analysis)

### **Console Outputs**
- [x] Class mapping printed
- [x] Train/test sizes printed
- [x] Model accuracies printed
- [x] Classification report printed
- [x] Cross-validation results printed
- [x] Business summary box printed

---

## üéØ Success Criteria

‚úÖ **Functionality**
- All 10 sections execute without errors
- Models train and predict successfully
- Charts save to disk correctly

‚úÖ **Accuracy**
- At least one model achieves >60% accuracy
- XGBoost expected to win (~65-75%)

‚úÖ **Reproducibility**
- Same results on every run (random_state=42)
- Cross-validation shows model stability

‚úÖ **Business Value**
- Feature importance identifies key drivers
- Insights are actionable and clear
- Summary box provides complete overview

---

## üîÆ Future Enhancements (Post-Hackathon)

### **Model Improvements**
- Hyperparameter tuning (GridSearchCV)
- Try neural networks (TensorFlow/PyTorch)
- Ensemble methods (voting classifier)

### **Feature Engineering**
- Create interaction features
- Polynomial features for non-linearity
- Feature scaling (StandardScaler)

### **Advanced Analysis**
- SHAP values for model interpretability
- ROC curves for each class (OvR)
- Learning curves to detect overfitting

### **Production Readiness**
- Save best model (pickle/joblib)
- Create prediction API (Flask/FastAPI)
- Deploy to cloud (AWS/GCP/Azure)

---

## üìù Author Notes

**Development Date**: February 2026  
**Hackathon**: Innofusion'26  
**Section**: 4 - Machine Learning  
**Environment**: Google Colab  
**Coding Style**: Beginner-friendly, sequential, well-commented

---

## üìû Support

For questions or issues:
1. Check section-by-section outputs
2. Verify dataset file name matches code
3. Ensure Google Colab has sufficient resources
4. Review error messages for specific issues

---

**End of Architecture Document**
